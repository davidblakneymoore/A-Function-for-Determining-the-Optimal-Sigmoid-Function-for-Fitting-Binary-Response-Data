
# A Function for Determining the Optimal Sigmoid Function for Fitting Binary
# Response Data

# David Moore
# davidblakneymoore@gmail.com
# May 2022


# The Explanation

# Though logistic regression is almost exclusively used for modeling
# probabilities based on binary response data, there are many other options.
# This function fits ten separate models to binary response data and
# determines which of the ten are best based on the residual sums of squares.
# The ten model types are based on the logistic function, the hyperbolic
# tangent, the arctangent function, the Gudermannian function, the error
# function, a generalised logistic function, an algebraic function, a more
# general algebraic function, the Gompertz function, and the Gompertz function
# after it has been rotated by 180 degrees. All ten of these functions have
# been rescaled so that they are bounded by 0 and 1 on the response variable
# axis. Please note that the Gompertz function and the rotated Gompertz
# function are the only functions out of the ten that are not radially
# symmetric about their inflection points - one side of these functions
# approaches the asymptote more gradually than the other.

# This function uses the 'R2jags' package heavily and it returns all the
# pertinent information for each model. For each model, it returns the model
# name, the actual model (with the parameters that best fit the data included),
# a data frame of fitted predictors and responses for plotting, the residual
# sum of squares, and the output from the Bayesian analysis, which includes all
# the coefficients and the Bayesian p value.

# Unfortunately, this function can't handle multiple predictor variables yet.
# Some day I may update it to be able to account for multiple predictor
# variables.

# I took an outstanding Bayesian statistics course with Remington Moll at the
# University of New Hampshire that helped me write this function.

# Negative pseudo r-squared values occur sometimes. When a straight line
# representing the average response fits the data better than the curve
# generated by this function, the pseudo r-squared value will be negative.

# This function takes 8 arguments. The first two are required.

# 'Predictor' is a vector of predictor variables to be used in the model.

# 'Response' is a vector of response variables to be used in the model. It
# should be binary (1s and 0s).

# 'Data_Frame' is an optional data frame to include such that column names can
# be supplied for the 'Predictor' and the 'Response' arguments. The data frame
# that these columns are from should be provided for this Data_Frame argument.

# 'Number_of_Iterations = 100000' is the number of iterations each Markov chain
# Monte Carlo simulation undergoes. The default is '1000'. More iterations lead
# to more precise results, but the program will take longer to run. I've found
# that doing 100000 iterations generally allows all the models to converge -
# doing 10000 generally isn't quite enough.

# 'Thinning_Rate = 1' describes how many iterations of the Markov chain Monte
# Carlo simulation are performed per stored value. The default is '1'. For
# example, if the thinning rate is 3, every third iteration of the Markov chain
# Monte Carlo simulation would be stored as model output.

# 'Burn_in_Value = 1000' is the number of initial iterations of each Markov
# chain Monte Carlo simulation that are discarded. Typically, it takes several
# iterations for parameter estimates to stabilize, so it is worthwhile to
# discard the first several iterations so they are not used in the final
# parameter estimates. It is often worth looking at plots of how parameter
# estimates change with Markov chain Monte Carlo iteration to ensure enough
# initial iterations are discarded and parameter estimates stabilize properly.

# 'Number_of_Chains = 3' is the number of separate Markov chain Monte Carlo
# iterations that will be run. The default, '3', is common, and fewer than 3 is
# not recommended.

# 'Working_Directory = getwd()' is the working directory in which to save the
# .txt files used by the 'R2jags::jags()' function.


# The Function

Function_for_Fitting_an_Optimal_Sigmoid_Model <- function (Predictor, Response, Data_Frame, Number_of_Iterations = 100000, Thinning_Rate = 1, Burn_in_Value = 1000, Number_of_Chains = 3, Working_Directory = getwd()) {
  
  # Prepare the Data
  
  Predictor_Name <- deparse(substitute(Predictor))
  Response_Name <- deparse(substitute(Response))
  if (!missing(Data_Frame)) {
    if (class(Data_Frame) != 'data.frame') {
      stop ("'Data_Frame' must be of class 'data.frame'.")
    }
    Data_Frame <- Data_Frame[, c(Predictor_Name, Response_Name)]
  } else if (missing(Data_Frame)) {
    Data_Frame <- data.frame(Predictor = Predictor, Response = Response)
  }
  Predictor <- Data_Frame$Predictor
  Response <- Data_Frame$Response
  Data <- list(Predictor = Data_Frame$Predictor, Response = Data_Frame$Response, Number_of_Observations = nrow(Data_Frame))
  Initial_Values <- function () {
    list()
  }
  Fitted_Predictor_Values <- seq(Minimum_Predictor_Value, Maximum_Predictor_Value, length.out = 1000000)
  
  
  # Meet Some Initial Conditions
  
  if (!is.integer(Predictor) & !is.numeric(Predictor)) {
    stop ("The 'Predictor' argument must be of class 'integer' or 'numeric'.")
  }
  if ((!is.integer(Response) & !is.logical(Response) & !is.numeric(Response)) & ((all(Response %in% c(0, 1))) | (all(Response %in% c(T, F))))) {
    stop ("The 'Response' argument must be of class 'integer', 'logical', or 'numeric' and it must only contain '1's and '0's or 'TRUE's and 'FALSE's.")
  }
  if ((length(Number_of_Iterations) != 1) | (!is.integer(Number_of_Iterations) & !is.numeric(Number_of_Iterations))) {
    stop ("The 'Number_of_Iterations' argument must be of class 'integer' or 'numeric' and it must be of length 1.")
  }
  if (length(Thinning_Rate) != 1 | (!is.integer(Thinning_Rate) & !is.numeric(Thinning_Rate))) {
    stop ("The 'Thinning_Rate' argument must be of class 'integer' or 'numeric' and it must be of length 1.")
  }
  if (length(Burn_in_Value) != 1 | (!is.integer(Burn_in_Value) & !is.numeric(Burn_in_Value))) {
    stop ("The 'Burn_in_Value' argument must be of class 'integer' or 'numeric' and it must be of length 1.")
  }
  if (length(Number_of_Chains) != 1 | (!is.integer(Number_of_Chains) & !is.numeric(Number_of_Chains))) {
    stop ("The 'Number_of_Chains' argument must be of class 'integer' or 'numeric' and it must be of length 1.")
  }
  if (!is.character(Working_Directory) | !file.exists(Working_Directory)) {
    stop ("The 'Working_Directory' argument must be a character vector and it must be a valid path to a folder on this computer.")
  }
  
  
  # Calculating the Total Sum of Squares
  
  Total_Sum_of_Squares <- sum((Response - mean(Response)) ^ 2)
  
  
  # Generating a Logistic Function Model
  
  # Response = (1 / (1 + exp(-(Intercept + (Slope * Predictor)))))
  
  cat("\nLogistic Function Model (one of ten)\n\n")
  Logistic_Function_Model_Name <- "Logistic Function"
  Lowercase_Logistic_Function_Model_Name <- "logistic function"
  sink("Logistic Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- (1 / (1 + exp(-(Intercept + (Slope * Predictor[i])))))
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Logistic_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Logistic Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Logistic_Function_Model_Response_Values <- (1 / (1 + exp(-(as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values)))))
  Logistic_Function_Model_Residual_Sum_of_Squares <- sum((Response - (1 / (1 + exp(-(as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor)))))) ^ 2)
  Logistic_Function_Model_Pseudo_R_Squared <- 1 - (Logistic_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Logistic_Function_Model <- paste0(Response_Name, " = (1 / (1 + exp(-(", as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")))))")
  Logistic_Function_Model_Bayesian_p_Value <- as.numeric(Logistic_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Logistic_Function_Model_Information <- list(Model_Name = Logistic_Function_Model_Name, Lowercase_Model_Name = Lowercase_Logistic_Function_Model_Name, Model = Logistic_Function_Model, Residual_Sum_of_Squares = Logistic_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Logistic_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Logistic_Function_Model_Response_Values, Output = Logistic_Function_Model_Output, Bayesian_p_Value = Logistic_Function_Model_Bayesian_p_Value)
  
  
  # Generating a Hyperbolic Tangent Model
  
  # Response = ((0.5 * tanh(Intercept + (Slope * Predictor))) + 0.5)
  
  cat("\n\nHyperbolic Tangent Model (two of ten)\n\n")
  Hyperbolic_Tangent_Model_Name <- "Hyperbolic Tangent"
  Lowercase_Hyperbolic_Tangent_Model_Name <- "hyperbolic tangent"
  sink("Hyperbolic Tangent Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((0.5 * tanh(Intercept + (Slope * Predictor[i]))) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Hyperbolic_Tangent_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Hyperbolic Tangent Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Hyperbolic_Tangent_Model_Response_Values <- ((0.5 * tanh(as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values)) + 0.5)
  Hyperbolic_Tangent_Model_Residual_Sum_of_Squares <- sum((Response - ((0.5 * tanh(as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Slope) * Predictor)) + 0.5)) ^ 2)
  Hyperbolic_Tangent_Model_Pseudo_R_Squared <- 1 - (Hyperbolic_Tangent_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Hyperbolic_Tangent_Model <- paste0(Response_Name, " = ((0.5 * tanh(", as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, "))) + 0.5)")
  Hyperbolic_Tangent_Model_Bayesian_p_Value <- as.numeric(Hyperbolic_Tangent_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Hyperbolic_Tangent_Model_Information <- list(Model_Name = Hyperbolic_Tangent_Model_Name, Lowercase_Model_Name = Lowercase_Hyperbolic_Tangent_Model_Name, Model = Hyperbolic_Tangent_Model, Residual_Sum_of_Squares = Hyperbolic_Tangent_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Hyperbolic_Tangent_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Hyperbolic_Tangent_Model_Response_Values, Output = Hyperbolic_Tangent_Model_Output, Bayesian_p_Value = Hyperbolic_Tangent_Model_Bayesian_p_Value)
  
  
  # Generating an Arctangent Function Model
  
  # Response = ((0.5 * ((2 / pi) * atan((pi / 2) * (Intercept + (Slope * Predictor))))) + 0.5)
  
  cat("\n\nArctangent Function Model (three of ten)\n\n")
  Arctangent_Function_Model_Name <- "Arctangent Function"
  Lowercase_Arctangent_Function_Model_Name <- "arctangent function"
  sink("Arctangent Function Model.txt")
  cat("model {
  
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    pi <- 3.14159265359
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((0.5 * ((2 / pi) * atan((pi / 2) * (Intercept + (Slope * Predictor[i]))))) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Arctangent_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Arctangent Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Arctangent_Function_Model_Response_Values <- ((0.5 * ((2 / pi) * atan((pi / 2) * (as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values)))) + 0.5)
  Arctangent_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((0.5 * ((2 / pi) * atan((pi / 2) * (as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor)))) + 0.5)) ^ 2)
  Arctangent_Function_Model_Pseudo_R_Squared <- 1 - (Arctangent_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Arctangent_Function_Model <- paste0(Response_Name, " = ((0.5 * ((2 / pi) * atan((pi / 2) * (", as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, "))))) + 0.5)")
  Arctangent_Function_Model_Bayesian_p_Value <- as.numeric(Arctangent_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Arctangent_Function_Model_Information <- list(Model_Name = Arctangent_Function_Model_Name, Lowercase_Model_Name = Lowercase_Arctangent_Function_Model_Name, Model = Arctangent_Function_Model, Residual_Sum_of_Squares = Arctangent_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Arctangent_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Arctangent_Function_Model_Response_Values, Output = Arctangent_Function_Model_Output, Bayesian_p_Value = Arctangent_Function_Model_Bayesian_p_Value)
  
  
  # Generating a Gudermannian Function Model
  
  # Response = ((2 / pi) * atan(tanh((Intercept + (Slope * Predictor)) * pi / 4)) + 0.5)
  
  cat("\n\nGudermannian Function Model (four of ten)\n\n")
  Gudermannian_Function_Model_Name <- "Gudermannian Function"
  Lowercase_Gudermannian_Function_Model_Name <- "Gudermannian function"
  sink("Gudermannian Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    pi <- 3.14159265359
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((2 / pi) * atan(tanh((Intercept + (Slope * Predictor[i])) * pi / 4)) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Gudermannian_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Gudermannian Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Gudermannian_Function_Model_Response_Values <- ((2 / pi) * atan(tanh((as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values) * pi / 4)) + 0.5)
  Gudermannian_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((2 / pi) * atan(tanh((as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor) * pi / 4)) + 0.5)) ^ 2)
  Gudermannian_Function_Model_Pseudo_R_Squared <- 1 - (Gudermannian_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Gudermannian_Function_Model <- paste0(Response_Name, " = ((2 / pi) * atan(tanh((", as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) * pi / 4)) + 0.5)")
  Gudermannian_Function_Model_Bayesian_p_Value <- as.numeric(Gudermannian_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Gudermannian_Function_Model_Information <- list(Model_Name = Gudermannian_Function_Model_Name, Lowercase_Model_Name = Lowercase_Gudermannian_Function_Model_Name, Model = Gudermannian_Function_Model, Residual_Sum_of_Squares = Gudermannian_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Gudermannian_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Gudermannian_Function_Model_Response_Values, Output = Gudermannian_Function_Model_Output, Bayesian_p_Value = Gudermannian_Function_Model_Bayesian_p_Value)
  
  
  # Generating an Error Function Model
  
  # Response = ((0.5 * ((2 * pnorm((Intercept + (Slope * Predictor)) * sqrt(2), 0, 1)) - 1)) + 0.5)
  
  cat("\n\nError Function Model (five of ten)\n\n")
  Error_Function_Model_Name <- "Error Function"
  Lowercase_Error_Function_Model_Name <- "error function"
  sink("Error Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    pi <- 3.14159265359
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((0.5 * ((2 * pnorm((Intercept + (Slope * Predictor[i])) * sqrt(2), 0, 1)) - 1)) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Error_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Error Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Error_Function_Model_Response_Values <- ((0.5 * ((2 * pnorm((as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values)) * sqrt(2))) - 1)) + 0.5)
  Error_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((0.5 * ((2 * pnorm((as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor)) * sqrt(2))) - 1)) + 0.5)) ^ 2)
  Error_Function_Model_Pseudo_R_Squared <- 1 - (Error_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Error_Function_Model <- paste0(Response_Name, " = ((0.5 * ((2 * pnorm((", as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Intercept)," + (", as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) * sqrt(2))) - 1)) + 0.5)")
  Error_Function_Model_Bayesian_p_Value <- as.numeric(Error_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Error_Function_Model_Information <- list(Model_Name = Error_Function_Model_Name, Lowercase_Model_Name = Lowercase_Error_Function_Model_Name, Model = Error_Function_Model, Residual_Sum_of_Squares = Error_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Error_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Error_Function_Model_Response_Values, Output = Error_Function_Model_Output, Bayesian_p_Value = Error_Function_Model_Bayesian_p_Value)
  
  
  # Generating a Generalised Logistic Function Model
  
  # Response = ((1 + exp(-(Intercept + (Slope * Predictor)))) ^ (-Exponent))
  
  cat("\n\nGeneralised Logistic Function Model (six of ten)\n\n")
  Generalised_Logistic_Function_Model_Name <- "Generalised Logistic Function"
  Lowercase_Generalised_Logistic_Function_Model_Name <- "generalised logistic function"
  sink("Generalised Logistic Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Exponent ~ dlnorm(0, 1)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((1 + exp(-(Intercept + (Slope * Predictor[i])))) ^ (-Exponent))
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Exponent", "Bayesian_p_Value")
  Generalised_Logistic_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Generalised Logistic Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Generalised_Logistic_Function_Model_Response_Values <- ((1 + exp(-(as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values))) ^ (-as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Exponent)))
  Generalised_Logistic_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((1 + exp(-(as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor))) ^ (-as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Exponent)))) ^ 2)
  Generalised_Logistic_Function_Model_Pseudo_R_Squared <- 1 - (Generalised_Logistic_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Generalised_Logistic_Function_Model <- paste0(Response_Name, " = ((1 + exp(-(", as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")))) ^ (-", as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Exponent), "))")
  Generalised_Logistic_Function_Model_Bayesian_p_Value <- as.numeric(Generalised_Logistic_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Generalised_Logistic_Function_Model_Information <- list(Model_Name = Generalised_Logistic_Function_Model_Name, Lowercase_Model_Name = Lowercase_Generalised_Logistic_Function_Model_Name, Model = Generalised_Logistic_Function_Model, Residual_Sum_of_Squares = Generalised_Logistic_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Generalised_Logistic_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Generalised_Logistic_Function_Model_Response_Values, Output = Generalised_Logistic_Function_Model_Output, Bayesian_p_Value = Generalised_Logistic_Function_Model_Bayesian_p_Value)
  
  
  # Generating an Algebraic Function Model
  
  # Response = ((0.5 * ((Intercept + (Slope * Predictor)) / sqrt(1 + ((Intercept + (Slope * Predictor)) ^ 2)))) + 0.5)
  
  cat("\n\nAlgebraic Function Model (seven of ten)\n\n")
  Algebraic_Function_Model_Name <- "Algebraic Function"
  Lowercase_Algebraic_Function_Model_Name <- "algebraic function"
  sink("Algebraic Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((0.5 * ((Intercept + (Slope * Predictor[i])) / sqrt(1 + ((Intercept + (Slope * Predictor[i])) ^ 2)))) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Algebraic_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Algebraic Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Algebraic_Function_Model_Response_Values <- ((0.5 * ((as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values) / sqrt(1 + ((as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values) ^ 2)))) + 0.5)
  Algebraic_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((0.5 * ((as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor) / sqrt(1 + ((as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor) ^ 2)))) + 0.5)) ^ 2)
  Algebraic_Function_Model_Pseudo_R_Squared <- 1 - (Algebraic_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Algebraic_Function_Model <- paste0(Response_Name, " = ((0.5 * ((", as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) / sqrt(1 + ((", as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) ^ 2)))) + 0.5)")
  Algebraic_Function_Model_Bayesian_p_Value <- as.numeric(Algebraic_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Algebraic_Function_Model_Information <- list(Model_Name = Algebraic_Function_Model_Name, Lowercase_Model_Name = Lowercase_Algebraic_Function_Model_Name, Model = Algebraic_Function_Model, Residual_Sum_of_Squares = Algebraic_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Algebraic_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Algebraic_Function_Model_Response_Values, Output = Algebraic_Function_Model_Output, Bayesian_p_Value = Algebraic_Function_Model_Bayesian_p_Value)
  
  
  # Generating a More General Algebraic Function Model
  
  # Response = ((0.5 * ((Intercept + (Slope * Predictor)) / ((1 + (abs(Intercept + (Slope * Predictor)) ^ Exponent)) ^ (1 / Exponent)))) + 0.5)
  
  cat("\n\nA More General Algebraic Function Model (eight of ten)\n\n")
  A_More_General_Algebraic_Function_Model_Name <- "A More General Algebraic Function"
  Lowercase_A_More_General_Algebraic_Function_Model_Name <- "a more general algebraic function"
  sink("A More General Algebraic Function Model.txt")
  cat("model {
  
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Exponent ~ dlnorm(0, 1)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- ((0.5 * ((Intercept + (Slope * Predictor[i])) / ((1 + (abs(Intercept + (Slope * Predictor[i])) ^ Exponent)) ^ (1 / Exponent)))) + 0.5)
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Exponent", "Bayesian_p_Value")
  A_More_General_Algebraic_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "A More General Algebraic Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_A_More_General_Algebraic_Function_Model_Response_Values <- ((0.5 * ((as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values) / ((1 + (abs(as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values) ^ as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent))) ^ (1 / as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent))))) + 0.5)
  A_More_General_Algebraic_Function_Model_Residual_Sum_of_Squares <- sum((Response - ((0.5 * ((as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor) / ((1 + (abs(as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept) + as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor) ^ as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent))) ^ (1 / as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent))))) + 0.5)) ^ 2)
  A_More_General_Algebraic_Logistic_Function_Model_Pseudo_R_Squared <- 1 - (A_More_General_Algebraic_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  A_More_General_Algebraic_Function_Model <- paste0(Response_Name, " = ((0.5 * ((", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) / ((1 + (abs(", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")) ^ ", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent), ")) ^ (1 / ", as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Exponent), ")))) + 0.5)")
  A_More_General_Algebraic_Function_Model_Bayesian_p_Value <- as.numeric(A_More_General_Algebraic_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  A_More_General_Algebraic_Function_Model_Information <- list(Model_Name = A_More_General_Algebraic_Function_Model_Name, Lowercase_Model_Name = Lowercase_A_More_General_Algebraic_Function_Model_Name, Model = A_More_General_Algebraic_Function_Model, Residual_Sum_of_Squares = A_More_General_Algebraic_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = A_More_General_Algebraic_Logistic_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_A_More_General_Algebraic_Function_Model_Response_Values, Output = A_More_General_Algebraic_Function_Model_Output, Bayesian_p_Value = A_More_General_Algebraic_Function_Model_Bayesian_p_Value)
  
  
  # Generating a Gompertz Function Model
  
  # Response = (exp(-exp(Intercept + (Slope * Predictor))))
  
  cat("\n\nGompertz Function Model (nine of ten)\n\n")
  Gompertz_Function_Model_Name <- "Gompertz Function"
  Lowercase_Gompertz_Function_Model_Name <- "Gompertz function"
  sink("Gompertz Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- (exp(-exp(Intercept + (Slope * Predictor[i]))))
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Gompertz_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Gompertz Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Gompertz_Function_Model_Response_Values <- (exp(-exp(as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values))))
  Gompertz_Function_Model_Residual_Sum_of_Squares <- sum((Response - (exp(-exp(as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor))))) ^ 2)
  Gompertz_Function_Model_Pseudo_R_Squared <- 1 - (Gompertz_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Gompertz_Function_Model <- paste0(Response_Name, " = (exp(-exp(", as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, "))))")
  Gompertz_Function_Model_Bayesian_p_Value <- as.numeric(Gompertz_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Gompertz_Function_Model_Information <- list(Model_Name = Gompertz_Function_Model_Name, Lowercase_Model_Name = Lowercase_Gompertz_Function_Model_Name, Model = Gompertz_Function_Model, Residual_Sum_of_Squares = Gompertz_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Gompertz_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Gompertz_Function_Model_Response_Values, Output = Gompertz_Function_Model_Output, Bayesian_p_Value = Gompertz_Function_Model_Bayesian_p_Value)
  
  
  # Generating a Gompertz Function Model That Has Been Rotated by 180 Degrees
  
  # Response = (1 - (exp(-exp(Intercept + (Slope * Predictor)))))
  
  cat("\n\nRotated Gompertz Function Model (ten of ten)\n\n")
  Rotated_Gompertz_Function_Model_Name <- "Rotated Gompertz Function"
  Lowercase_Rotated_Gompertz_Function_Model_Name <- "rotated Gompertz function"
  sink("Rotated Gompertz Function Model.txt")
  cat("model {
    
    # Priors
    Intercept ~ dnorm(0, 0.001)
    Slope ~ dnorm(0, 0.001)
    Sigma ~ dlnorm(0, 1)
    Tau <- (1 / (Sigma ^ 2))
    
    # Likelihood and Model Fit
    for (i in 1:Number_of_Observations) {
      Response[i] ~ dnorm(Mean[i], Tau)
      Mean[i] <- (1 - (exp(-exp(Intercept + (Slope * Predictor[i])))))
      Actual_Squared_Residual[i] <- ((Response[i] - Mean[i]) ^ 2)
      Simulated_Response[i] ~ dbern(Mean[i])
      Simulated_Squared_Residual[i] <- ((Simulated_Response[i] - Mean[i]) ^ 2)
    }
    Bayesian_p_Value <- step((sum((Simulated_Squared_Residual[]) ^ 2)) / (sum((Actual_Squared_Residual[]) ^ 2)) - 1) 
    
  }", fill = T)
  sink()
  Parameters <- c("Intercept", "Slope", "Bayesian_p_Value")
  Rotated_Gompertz_Function_Model_Output <- R2jags::jags(Data, Initial_Values, Parameters, "Rotated Gompertz Function Model.txt", n.chains = Number_of_Chains, n.thin = Thinning_Rate, n.iter = Number_of_Iterations, n.burnin = Burn_in_Value, working.directory = Working_Directory)
  Fitted_Rotated_Gompertz_Function_Model_Response_Values <- (1 - (exp(-exp(as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Slope) * Fitted_Predictor_Values)))))
  Rotated_Gompertz_Function_Model_Residual_Sum_of_Squares <- sum((Response - (1 - (exp(-exp(as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept) + (as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Slope) * Predictor)))))) ^ 2)
  Rotated_Gompertz_Function_Model_Pseudo_R_Squared <- 1 - (Rotated_Gompertz_Function_Model_Residual_Sum_of_Squares / Total_Sum_of_Squares)
  Rotated_Gompertz_Function_Model <- paste0(Response_Name, " = (1 - (exp(-exp(", as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Intercept), " + (", as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Slope), " * ", Predictor_Name, ")))))")
  Rotated_Gompertz_Function_Model_Bayesian_p_Value <- as.numeric(Rotated_Gompertz_Function_Model_Output$BUGSoutput$mean$Bayesian_p_Value)
  Rotated_Gompertz_Function_Model_Information <- list(Model_Name = Rotated_Gompertz_Function_Model_Name, Lowercase_Model_Name = Lowercase_Rotated_Gompertz_Function_Model_Name, Model = Rotated_Gompertz_Function_Model, Residual_Sum_of_Squares = Rotated_Gompertz_Function_Model_Residual_Sum_of_Squares, Pseudo_R_Squared = Rotated_Gompertz_Function_Model_Pseudo_R_Squared, Fitted_Response_Values = Fitted_Rotated_Gompertz_Function_Model_Response_Values, Output = Rotated_Gompertz_Function_Model_Output, Bayesian_p_Value = Rotated_Gompertz_Function_Model_Bayesian_p_Value)
  
  
  # Compiling the Models Into One List
  
  Model_List <- list(Logistic_Function_Model = Logistic_Function_Model_Information, Hyperbolic_Tangent_Model = Hyperbolic_Tangent_Model_Information, Arctangent_Function_Model = Arctangent_Function_Model_Information, Gudermannian_Function_Model = Gudermannian_Function_Model_Information, Error_Function_Model_Information = Error_Function_Model_Information, Generalised_Logistic_Function_Model = Generalised_Logistic_Function_Model_Information, Algebraic_Function_Model = Algebraic_Function_Model_Information, A_More_General_Algebraic_Function_Model = A_More_General_Algebraic_Function_Model_Information, Gompertz_Function_Model = Gompertz_Function_Model_Information, Rotated_Gompertz_Function_Model = Rotated_Gompertz_Function_Model_Information)
  
  
  # Returning the Pertinent Model Information
  
  Bayesian_Model_Metadata <- data.frame(Number_of_Iterations = Number_of_Iterations, Thinning_Rate = Thinning_Rate, Burn_in_Value = Burn_in_Value, Number_of_Chains = Number_of_Chains)
  Pertinent_Model_Information_List <- lapply(Model_List, function (x) {
    list(Model_Name = x$Model_Name, Model = x$Model, Fitted_Values = data.frame(Predictor = Fitted_Predictor_Values, Response = x$Fitted_Response_Values), Residual_Sum_of_Squares = x$Residual_Sum_of_Squares, Pseudo_R_Squared = x$Pseudo_R_Squared, Output = x$Output$BUGSoutput$summary)
  })
  Conclusion <- paste0("The model that best fits the data is the ", unlist(sapply(Model_List, `[`, 'Lowercase_Model_Name'))[which.max(unlist(sapply(Model_List, `[`, 'Pseudo_R_Squared')))], " model.")
  Pertinent_Model_Information_List <- list(Bayesian_Model_Metadata = Bayesian_Model_Metadata, Model_Information = Pertinent_Model_Information_List, Conclusion = Conclusion)
  class(Pertinent_Model_Information_List) <- "Custom_Class"
  return (Pertinent_Model_Information_List)

}

print.Custom_Class <- function (x) {
  cat("Function Output:\n\n")
  print(list(Bayesian_Model_Metadata = as.data.frame(x$Bayesian_Model_Metadata), Model_Information = lapply(Function_Output$Model_Information, `[`, c('Model_Name', 'Model', 'Residual_Sum_of_Squares', 'Pseudo_R_Squared', 'Output')), Conclusion = x$Conclusion))
}


# An Example

# Generating Some Practice Data

Number_of_Observations <- 100
Minimum_Predictor_Value <- 0
Maximum_Predictor_Value <- 25
Predictor_Variable <- runif(Number_of_Observations, Minimum_Predictor_Value, Maximum_Predictor_Value)
Response_Variable <- rbinom(Number_of_Observations, 1, (Predictor_Variable - Minimum_Predictor_Value) / (Maximum_Predictor_Value - Minimum_Predictor_Value))
Data_Frame <- data.frame(Predictor_Variable = Predictor_Variable, Response_Variable = Response_Variable)

# Test the Function Out

(Function_Output <- Function_for_Fitting_an_Optimal_Sigmoid_Model(Predictor_Variable, Response_Variable, Data_Frame))

# Here's the output from the preceding line of code.

# Be patient - it may take a while.

# > (Function_Output <- Function_for_Fitting_an_Optimal_Sigmoid_Model(Predictor_Variable, Response_Variable, Data_Frame))
# 
# Logistic Function Model (one of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1319
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Hyperbolic Tangent Model (two of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1220
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Arctangent Function Model (three of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1423
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Gudermannian Function Model (four of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1523
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Error Function Model (five of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1522
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Generalised Logistic Function Model (six of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 104
# Total graph size: 1321
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Algebraic Function Model (seven of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1520
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# A More General Algebraic Function Model (eight of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 104
# Total graph size: 1622
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Gompertz Function Model (nine of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1219
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# 
# 
# Rotated Gompertz Function Model (ten of ten)
# 
# Compiling model graph
# Resolving undeclared variables
# Allocating nodes
# Graph information:
#   Observed stochastic nodes: 100
# Unobserved stochastic nodes: 103
# Total graph size: 1319
# 
# Initializing model
# 
# |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
# |**************************************************| 100%
# Function Output:
#   
# $Bayesian_Model_Metadata
#   Number_of_Iterations Thinning_Rate Burn_in_Value Number_of_Chains
# 1                1e+05             1          1000                3
# 
# $Model_Information
# $Model_Information$Logistic_Function_Model
# $Model_Information$Logistic_Function_Model$Model_Name
# [1] "Logistic Function"
# 
# $Model_Information$Logistic_Function_Model$Model
# [1] "Response_Variable = (1 / (1 + exp(-(-2.81744169092709 + (0.274935457509424 * Predictor_Variable)))))"
# 
# $Model_Information$Logistic_Function_Model$Residual_Sum_of_Squares
# [1] 14.75785
# 
# $Model_Information$Logistic_Function_Model$Pseudo_R_Squared
# [1] 0.409686
# 
# $Model_Information$Logistic_Function_Model$Output
#                        mean         sd       2.5%        25%        50%        75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.5071481 0.49994974  0.0000000  0.0000000  1.0000000  1.0000000   1.0000000 1.001007 150000
# Intercept        -2.8174417 0.69815460 -4.4291302 -3.1778472 -2.7277689 -2.3468186  -1.7418191 1.001029  57000
# Slope             0.2749355 0.07265096  0.1712537  0.2268861  0.2636905  0.3089492   0.4460152 1.001013 100000
# deviance         95.4566008 2.84447886 92.2966183 93.4139409 94.6995766 96.6833949 102.8879521 1.001076  24000
# 
# 
# $Model_Information$Hyperbolic_Tangent_Model
# $Model_Information$Hyperbolic_Tangent_Model$Model_Name
# [1] "Hyperbolic Tangent"
# 
# $Model_Information$Hyperbolic_Tangent_Model$Model
# [1] "Response_Variable = ((0.5 * tanh(-1.41260228908666 + (0.137874943502405 * Predictor_Variable))) + 0.5)"
# 
# $Model_Information$Hyperbolic_Tangent_Model$Residual_Sum_of_Squares
# [1] 14.7604
# 
# $Model_Information$Hyperbolic_Tangent_Model$Pseudo_R_Squared
# [1] 0.409584
# 
# $Model_Information$Hyperbolic_Tangent_Model$Output
#                        mean         sd        2.5%        25%        50%        75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.5030640 0.49999145  0.00000000  0.0000000  1.0000000  1.0000000   1.0000000 1.001018  82000
# Intercept        -1.4126023 0.35093525 -2.21472320 -1.5939652 -1.3663596 -1.1759539  -0.8726282 1.000994 300000
# Slope             0.1378749 0.03656028  0.08567786  0.1136327  0.1321272  0.1550594   0.2231141 1.000997 300000
# deviance         95.4761647 2.84236025 92.29904958 93.4219004 94.7320377 96.7283843 102.8516734 1.001005 180000
# 
# 
# $Model_Information$Arctangent_Function_Model
# $Model_Information$Arctangent_Function_Model$Model_Name
# [1] "Arctangent Function"
# 
# $Model_Information$Arctangent_Function_Model$Model
# [1] "Response_Variable = ((0.5 * ((2 / pi) * atan((pi / 2) * (-2.00777773926134 + (0.206495998490139 * Predictor_Variable))))) + 0.5)"
# 
# $Model_Information$Arctangent_Function_Model$Residual_Sum_of_Squares
# [1] 15.17498
# 
# $Model_Information$Arctangent_Function_Model$Pseudo_R_Squared
# [1] 0.3930009
# 
# $Model_Information$Arctangent_Function_Model$Output
#                        mean         sd       2.5%        25%       50%        75%       97.5%     Rhat n.eff
# Bayesian_p_Value  0.5356902 0.49872542  0.0000000  0.0000000  1.000000  1.0000000   1.0000000 1.001024 65000
# Intercept        -2.0077777 0.76749187 -3.8213449 -2.3192721 -1.865930 -1.5163508  -1.0369052 1.008910 13000
# Slope             0.2064960 0.08476862  0.1042556  0.1525072  0.189558  0.2392372   0.4088162 1.001027 59000
# deviance         98.0238215 3.04652662 94.6346365 95.8355518 97.227267 99.3459547 105.8408624 1.001124 15000
# 
# 
# $Model_Information$Gudermannian_Function_Model
# $Model_Information$Gudermannian_Function_Model$Model_Name
# [1] "Gudermannian Function"
# 
# $Model_Information$Gudermannian_Function_Model$Model
# [1] "Response_Variable = ((2 / pi) * atan(tanh((-1.47871602047179 + (0.145382457052783 * Predictor_Variable)) * pi / 4)) + 0.5)"
# 
# $Model_Information$Gudermannian_Function_Model$Residual_Sum_of_Squares
# [1] 14.81049
# 
# $Model_Information$Gudermannian_Function_Model$Pseudo_R_Squared
# [1] 0.4075805
# 
# $Model_Information$Gudermannian_Function_Model$Output
#                        mean         sd        2.5%        25%       50%       75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.5070000 0.49995184  0.00000000  0.0000000  1.000000  1.000000   1.0000000 1.000999 300000
# Intercept        -1.4787160 0.38621952 -2.37670304 -1.6719220 -1.424739 -1.218852  -0.8971377 1.001050  35000
# Slope             0.1453825 0.04059272  0.08837514  0.1185286  0.138724  0.163878   0.2420987 1.001021  74000
# deviance         95.7953553 2.88199089 92.59715219 93.7294556 95.030988 97.038412 103.3253803 1.001074  25000
# 
# 
# $Model_Information$Error_Function_Model_Information
# $Model_Information$Error_Function_Model_Information$Model_Name
# [1] "Error Function"
# 
# $Model_Information$Error_Function_Model_Information$Model
# [1] "Response_Variable = ((0.5 * ((2 * pnorm((-1.18986143065775 + (0.114991993224239 * Predictor_Variable)) * sqrt(2))) - 1)) + 0.5)"
# 
# $Model_Information$Error_Function_Model_Information$Residual_Sum_of_Squares
# [1] 14.69599
# 
# $Model_Information$Error_Function_Model_Information$Pseudo_R_Squared
# [1] 0.4121605
# 
# $Model_Information$Error_Function_Model_Information$Output
#                       mean         sd        2.5%         25%        50%        75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.488734 0.49987390  0.00000000  0.00000000  0.0000000  1.0000000   1.0000000 1.001005 170000
# Intercept        -1.189861 0.27208625 -1.81531795 -1.33843350 -1.1576538 -1.0033992  -0.7528651 1.001053  34000
# Slope             0.114992 0.02755671  0.07372324  0.09638159  0.1109226  0.1287157   0.1802113 1.001041  42000
# deviance         95.065747 2.79024700 91.92577699 93.03764109 94.3278866 96.3037426 102.3468686 1.001016  87000
# 
# 
# $Model_Information$Generalised_Logistic_Function_Model
# $Model_Information$Generalised_Logistic_Function_Model$Model_Name
# [1] "Generalised Logistic Function"
# 
# $Model_Information$Generalised_Logistic_Function_Model$Model
# [1] "Response_Variable = ((1 + exp(-(-4.93899228383167 + (0.387509734150288 * Predictor_Variable)))) ^ (-1.57180824990506))"
# 
# $Model_Information$Generalised_Logistic_Function_Model$Residual_Sum_of_Squares
# [1] 19.61971
# 
# $Model_Information$Generalised_Logistic_Function_Model$Pseudo_R_Squared
# [1] 0.2152115
# 
# $Model_Information$Generalised_Logistic_Function_Model$Output
#                        mean        sd         2.5%        25%        50%       75%       97.5%     Rhat n.eff
# Bayesian_p_Value  0.4952626 0.4999784   0.00000000  0.0000000  0.0000000  1.000000   1.0000000 1.001124 15000
# Exponent          1.5718082 2.2002392   0.09084056  0.4084910  0.8812828  1.844186   7.5023393 1.001567  3500
# Intercept        -4.9389923 6.2407231 -22.07698444 -5.8612255 -3.1188301 -1.544207   0.4251846 1.013605  8300
# Slope             0.3875097 0.3203627   0.15552350  0.2251631  0.2889806  0.416420   1.2534394 1.001624  3200
# deviance         96.0249069 3.1605804  91.73642531 93.7165998 95.4813900 97.612196 103.8355813 1.001552  3600
# 
# 
# $Model_Information$Algebraic_Function_Model
# $Model_Information$Algebraic_Function_Model$Model_Name
# [1] "Algebraic Function"
# 
# $Model_Information$Algebraic_Function_Model$Model
# [1] "Response_Variable = ((0.5 * ((-1.60017751347136 + (0.159766973086163 * Predictor_Variable)) / sqrt(1 + ((-1.60017751347136 + (0.159766973086163 * Predictor_Variable)) ^ 2)))) + 0.5)"
# 
# $Model_Information$Algebraic_Function_Model$Residual_Sum_of_Squares
# [1] 14.92225
# 
# $Model_Information$Algebraic_Function_Model$Pseudo_R_Squared
# [1] 0.40311
# 
# $Model_Information$Algebraic_Function_Model$Output
#                        mean         sd        2.5%        25%        50%       75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.5239865 0.49942516  0.00000000  0.0000000  1.0000000  1.000000   1.0000000 1.001043  40000
# Intercept        -1.6001775 0.45644225 -2.71460620 -1.8254262 -1.5275838 -1.289088  -0.9276276 1.001159  12000
# Slope             0.1597670 0.04914004  0.09193498  0.1265451  0.1507463  0.182149   0.2828056 1.001166  12000
# deviance         96.4747196 2.88983017 93.21655464 94.3691383 95.7121345 97.760910 104.0761286 1.001001 260000
# 
# 
# $Model_Information$A_More_General_Algebraic_Function_Model
# $Model_Information$A_More_General_Algebraic_Function_Model$Model_Name
# [1] "A More General Algebraic Function"
# 
# $Model_Information$A_More_General_Algebraic_Function_Model$Model
# [1] "Response_Variable = ((0.5 * ((-23.9765901206144 + (2.65596291237577 * Predictor_Variable)) / ((1 + (abs(-23.9765901206144 + (2.65596291237577 * Predictor_Variable)) ^ 0.83929451283207)) ^ (1 / 0.83929451283207)))) + 0.5)"
# 
# $Model_Information$A_More_General_Algebraic_Function_Model$Residual_Sum_of_Squares
# [1] 17.97335
# 
# $Model_Information$A_More_General_Algebraic_Function_Model$Pseudo_R_Squared
# [1] 0.281066
# 
# $Model_Information$A_More_General_Algebraic_Function_Model$Output
#                         mean         sd        2.5%         25%         50%         75%      97.5%     Rhat  n.eff
# Bayesian_p_Value   0.5791414  0.4936977   0.0000000   0.0000000   1.0000000   1.0000000   1.000000 1.001004 200000
# Exponent           0.8392945  1.2636013   0.3558036   0.4416266   0.5184998   0.6973387   3.668798 1.013202    320
# Intercept        -23.9765901 19.2978776 -68.0242038 -36.6986172 -20.1004834  -7.3295238  -1.153962 1.012830    260
# Slope              2.6559629  2.2271423   0.1106941   0.7758893   2.1737194   4.0249643   7.872490 1.008056    290
# deviance         102.1325543  3.6197620  93.7682184 100.4784282 102.2728905 104.1006202 109.399640 1.005364    520
# 
# 
# $Model_Information$Gompertz_Function_Model
# $Model_Information$Gompertz_Function_Model$Model_Name
# [1] "Gompertz Function"
# 
# $Model_Information$Gompertz_Function_Model$Model
# [1] "Response_Variable = (exp(-exp(1.57659806974731 + (-0.20014343206286 * Predictor_Variable))))"
# 
# $Model_Information$Gompertz_Function_Model$Residual_Sum_of_Squares
# [1] 14.569
# 
# $Model_Information$Gompertz_Function_Model$Pseudo_R_Squared
# [1] 0.4172402
# 
# $Model_Information$Gompertz_Function_Model$Output
#                        mean         sd       2.5%        25%        50%        75%       97.5%     Rhat  n.eff
# Bayesian_p_Value  0.4568215 0.49813297  0.0000000  0.0000000  0.0000000  1.0000000   1.0000000 1.001112  17000
# Intercept         1.5765981 0.47467326  0.8644013  1.2542606  1.5061923  1.8172120   2.6927280 1.001149  13000
# Slope            -0.2001434 0.05324085 -0.3268541 -0.2250246 -0.1911818 -0.1646408  -0.1249099 1.001107  18000
# deviance         94.1598982 2.87350983 90.9605163 92.0859205 93.4041617 95.4114713 101.6443834 1.001010 120000
# 
# 
# $Model_Information$Rotated_Gompertz_Function_Model
# $Model_Information$Rotated_Gompertz_Function_Model$Model_Name
# [1] "Rotated Gompertz Function"
# 
# $Model_Information$Rotated_Gompertz_Function_Model$Model
# [1] "Response_Variable = (1 - (exp(-exp(-2.3549722339602 + (0.180805641514572 * Predictor_Variable)))))"
# 
# $Model_Information$Rotated_Gompertz_Function_Model$Residual_Sum_of_Squares
# [1] 14.95632
# 
# $Model_Information$Rotated_Gompertz_Function_Model$Pseudo_R_Squared
# [1] 0.401747
# 
# $Model_Information$Rotated_Gompertz_Function_Model$Output
#                        mean         sd       2.5%        25%        50%        75%       97.5%     Rhat n.eff
# Bayesian_p_Value  0.4930000 0.49995184  0.0000000  0.0000000  0.0000000  1.0000000   1.0000000 1.001055 33000
# Intercept        -2.3549722 0.50367745 -3.5072962 -2.6176403 -2.2918178 -2.0146593  -1.5730672 1.001351  5600
# Slope             0.1808056 0.04748051  0.1139822  0.1498772  0.1734703  0.2020339   0.2927681 1.001313  6300
# deviance         96.8267193 2.90793260 93.6085590 94.7305963 96.0509754 98.0798653 104.4973258 1.001061 30000
# 
# 
# 
# $Conclusion
# [1] "The model that best fits the data is the Gompertz function model."


# Generating a Plot of All the Models

# Make sure to expand the plotting window.

Color <- rainbow(length(Function_Output$Model_Information))
par(mar = c(12, 4, 4, 2))
plot(Response_Variable ~ Predictor_Variable, Data_Frame, main = "Fitting Sigmoid Models to the Data", xlab = "", ylab = "", pch = 19, type = 'n')
title(xlab = "Predictor Variable", line = 2.5)
title(ylab = "Response Variable", line = 2.5)
for (i in seq_len(length(Function_Output$Model_Information))) {
  lines(Function_Output$Model_Information[[i]]$Fitted_Values$Response ~ Function_Output$Model_Information[[i]]$Fitted_Values$Predictor, col = Color[i], lwd = 2)
}
points(Response_Variable ~ Predictor_Variable, Data_Frame, pch = 19)
legend("bottom", xpd = T, ncol = 2, inset = c(0, -0.55), title = expression(paste("Model Type (Pseudo ", italic("R") ^ "2" * ")")), legend = paste0(unlist(sapply(Function_Output$Model_Information, `[`, 'Model_Name')), " (", format(round(unlist(sapply(Function_Output$Model_Information, `[`, 'Pseudo_R_Squared')), 3), nsmall = 3), ")"), col = Color, lty = 1, lwd = 2)


# Works Cited

# Su, Y.-S., and M. Yajima. 2021. R2jags: Using R to Run 'JAGS'. R package
# version 0.7-1. https://cran.r-project.org/web/packages/R2jags/.
